<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Brice Green" />

<meta name="date" content="2020-01-25" />

<title>Understanding Model Convergence</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Understanding Model Convergence</h1>
<h4 class="author">Brice Green</h4>
<h4 class="date">2020-01-25</h4>



<p>The <code>baggr</code> package makes running Bayesian meta-analysis models very easy, but that ease can also obscure some things that go on behind the scenes. Especially with low-data settings, say with only a few measurements, you may get cryptic error messages when you run your models, like</p>
<pre><code>Warning: There were 110 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup</code></pre>
<p>This vignette will walk through the basic intuition for what those mean, whether to be concerned about them, and how to tune the parameters underneath the hood. There are already a number of good resources that go more in depth on the topic so first I’ll point you there:</p>
<p><a href="https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html">Visual MCMC Diagnostics Using the Bayesplot Package</a></p>
<p><a href="https://mc-stan.org/docs/2_21/reference-manual/notation-for-samples-chains-and-draws.html">Scale Reduction in the Stan Manual</a></p>
<p><a href="https://mc-stan.org/docs/2_21/reference-manual/divergent-transitions.html">Divergent Transitions in the Stan Manual</a></p>
<p>These go more in depth than this discussion is going to, but if you find yourself needing a deeper discussion then these would be good starting points.</p>
<div id="behind-the-scenes-basics-of-mcmc" class="section level1">
<h1>Behind the scenes: basics of MCMC</h1>
<p>The baggr package uses fully Bayesian methods to estimate meta-analysis models, meaning that it takes a prior (which if not specified is assigned by default), and jointly with the data uses that prior to estimate a posterior according to Bayes’ rule</p>
<p><span class="math display">\[P(\theta | X) = \dfrac{P(X | \theta) P(\theta)}{P(X)}\]</span></p>
<p>In order to calculate <span class="math inline">\(P(X)\)</span>, we need some computationally expensive machinery that can calculate</p>
<p><span class="math display">\[P(X) = \int P(X | \theta) d\theta \]</span></p>
<p>which is (in general) really difficult except for a few specific situations. In order to make these types of models easy to run, we use a method that approximates this integral to a high degree of numerical accuracy.</p>
<p>MCMC methods leverage an accept/reject proposal scheme by generating a potential value for <span class="math inline">\(\theta\)</span>, which is some set of parmeters, and using the relative likelihoods (<span class="math inline">\(P(\theta|X)\)</span>) at that new value to determine the probability of whether to accept or reject those proposals. The simplest of these methods, Metropolis-Hastings, pretty much ends there. But for meta-analysis models (and other models with of high dimension), the spaces that the random walk has to explore get really complicated.</p>
<p>So instead of plain-old Metropolis-Hastings, we use Hamiltonian Monte Carlo with No-U-Turn sampling, powered on the back end by Stan. This algorithm leverages the gradient, or the rate of change in the posterior density at a given parameter value, in order to more efficiently generate new parameter proposals.</p>
</div>
<div id="warnings-like-divergent-transitions" class="section level1">
<h1>Warnings like “divergent transitions”</h1>
<p>The gradient is passed into a Hamiltonian equation, which simulates a path for where it thinks the posterior is headed. But this trajectory is based on the local behavior of the posterior, and depending on the geometry, if we move too far away from that point we might get more and more off base. Stan, on the back end, uses the <code>adapt_delta</code> parameter to determine how often it needs to check its simulated paths, re-evaluating the trajectory it is using.</p>
<p>If you make <code>adapt_delta</code> really big, you end up moving really slowly, but you also are less likely to fly off the true trajectory of the posterior distribution. But if you make it small, you go faster, etc. You get the idea. How high you should set the parameter depends on the extent to which the algorithm is properly modeling the paths. So if you encounter divergent transitions, it is often good to raise <code>adapt_delta</code>. This discussion is really oversimplifying things to build the intuition, but you can find an (accessible) deeper discussion <a href="https://arxiv.org/abs/1701.02434">here</a> if it is of interest!</p>
<p>When you get a warning that there are a certain number of “divergent transitions,” it means that the samples have gone far away from the anticipated path, and (at worse) are reduced to a random walk. This means that they may not be fully exploring the posterior distribution, and so you may need more iterations to get a good approximation of the distribution.</p>
<p>If you get this warning and want to re-estimate your model with a higher <code>adapt_delta</code> setting, you can change the argument in the call to the <code>baggr</code> function like so:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># the ... parameters in the call to baggr() are</span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="co"># passed to rstan::sampling</span></a>
<a class="sourceLine" id="cb2-3" title="3"><span class="co"># for documentation see ?rstan::sampling</span></a>
<a class="sourceLine" id="cb2-4" title="4"><span class="kw">library</span>(baggr)</a>
<a class="sourceLine" id="cb2-5" title="5"></a>
<a class="sourceLine" id="cb2-6" title="6">fit &lt;-<span class="st"> </span><span class="kw">baggr</span>(schools, <span class="dt">model =</span> <span class="st">&quot;rubin&quot;</span>, </a>
<a class="sourceLine" id="cb2-7" title="7">      <span class="dt">pooling =</span> <span class="st">&quot;partial&quot;</span>,</a>
<a class="sourceLine" id="cb2-8" title="8">      <span class="dt">prior_hypermean =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb2-9" title="9">      <span class="dt">prior_hypersd =</span> <span class="kw">cauchy</span>(<span class="dv">0</span>, <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb2-10" title="10">      <span class="dt">control =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb2-11" title="11">        <span class="dt">adapt_delta =</span> <span class="fl">0.9</span> <span class="co"># 0.8 by default</span></a>
<a class="sourceLine" id="cb2-12" title="12">      ))</a></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.098 seconds (Warm-up)
## Chain 1:                0.063 seconds (Sampling)
## Chain 1:                0.161 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.132 seconds (Warm-up)
## Chain 2:                0.062 seconds (Sampling)
## Chain 2:                0.194 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.064 seconds (Warm-up)
## Chain 3:                0.075 seconds (Sampling)
## Chain 3:                0.139 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.078 seconds (Warm-up)
## Chain 4:                0.079 seconds (Sampling)
## Chain 4:                0.157 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" title="1">fit</a></code></pre></div>
<pre><code>## Model type: Rubin model with aggregate data 
## Pooling of effects: partial 
## 
## Aggregate treatment effect (on mean):
## Hypermean (tau) =  6.5 with 95% interval -1.4 to 14.7 
## Hyper-SD (sigma_tau) = 2.838 with 95% interval 0.094 to 9.908 
## 
## Treatment effects on mean:
##          mean  sd pooling
## School A  7.6 5.4    0.95
## School B  6.6 4.9    0.90
## School C  6.1 5.3    0.95
## School D  6.6 4.8    0.91
## School E  5.7 4.9    0.89
## School F  6.0 5.0    0.91
## School G  7.7 5.2    0.90
## School H  6.8 5.3    0.96</code></pre>
<p>If you still get a lot of divergent transitions after raising the <code>adapt_delta</code> really high, it may indicate an issue with your model! This is what is commonly referred to as the <a href="https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/">folk theorem of statistical computing</a>.</p>
</div>
<div id="widehatr-being-too-low" class="section level1">
<h1><span class="math inline">\(\widehat{R}\)</span> being too low</h1>
<p>The other warning you get, that “r hat” statistics are too high, indicates that the effective sample size (ESS) of your draws are too low and your chains have not converged. Because MCMC is not a deterministic algorithm, we run multiple chains of samples in order to check that each has converged to a good enough answer.</p>
<p>We want to check that each chain is “mixing well,” meaning that we have enough effective draws after accounting for the correlation between subsequent samples. We also want to check that markov chains initialized at different values get to similar places. <a href="https://mc-stan.org/docs/2_21/reference-manual/notation-for-samples-chains-and-draws.html">Effective sample size</a> gives us a measure of the first, and <a href="https://mc-stan.org/docs/2_21/reference-manual/notation-for-samples-chains-and-draws.html"><span class="math inline">\(\widehat{R}\)</span></a> gives a measure of the second.</p>
<p>If you have a low ESS or high <span class="math inline">\(\widehat{R}\)</span>, you are best off running each chain for more iterations until you get a higher ESS/lower <span class="math inline">\(\widehat{R}\)</span>. To run chains for longer, you change the <code>iter</code> parameter in the <code>...</code> arguments to your <code>baggr()</code> call.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" title="1"><span class="co"># runs for 10,000 iterations per chain instead of 2,000</span></a>
<a class="sourceLine" id="cb6-2" title="2">fit &lt;-<span class="st"> </span><span class="kw">baggr</span>(schools, <span class="dt">model =</span> <span class="st">&quot;rubin&quot;</span>, <span class="dt">pooling =</span> <span class="st">&quot;partial&quot;</span>,</a>
<a class="sourceLine" id="cb6-3" title="3">      <span class="dt">prior_hypermean =</span> <span class="kw">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">prior_hypersd =</span> <span class="kw">cauchy</span>(<span class="dv">0</span>,<span class="dv">2</span>),</a>
<a class="sourceLine" id="cb6-4" title="4">      <span class="dt">iter =</span> <span class="dv">10000</span>, <span class="dt">control =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb6-5" title="5">        <span class="dt">adapt_delta =</span> <span class="fl">0.95</span> <span class="co"># like above, to address divergences</span></a>
<a class="sourceLine" id="cb6-6" title="6">      ))</a></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 1: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.437 seconds (Warm-up)
## Chain 1:                0.4 seconds (Sampling)
## Chain 1:                0.837 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 2: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.425 seconds (Warm-up)
## Chain 2:                0.348 seconds (Sampling)
## Chain 2:                0.773 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 3: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.383 seconds (Warm-up)
## Chain 3:                0.355 seconds (Sampling)
## Chain 3:                0.738 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;rubin&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 10000 [  0%]  (Warmup)
## Chain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)
## Chain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)
## Chain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)
## Chain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)
## Chain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)
## Chain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)
## Chain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)
## Chain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)
## Chain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)
## Chain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)
## Chain 4: Iteration: 10000 / 10000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.391 seconds (Warm-up)
## Chain 4:                0.377 seconds (Sampling)
## Chain 4:                0.768 seconds (Total)
## Chain 4:</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" title="1">fit</a></code></pre></div>
<pre><code>## Model type: Rubin model with aggregate data 
## Pooling of effects: partial 
## 
## Aggregate treatment effect (on mean):
## Hypermean (tau) =  0.4 with 95% interval -1.5 to 2.3 
## Hyper-SD (sigma_tau) = 2.456 with 95% interval 0.063 to 9.824 
## 
## Treatment effects on mean:
##          mean  sd pooling
## School A 1.56 3.9    0.96
## School B 1.02 3.1    0.92
## School C 0.27 3.3    0.96
## School D 0.83 3.1    0.93
## School E 0.26 2.9    0.91
## School F 0.47 3.1    0.93
## School G 1.79 3.7    0.92
## School H 0.78 3.5    0.97</code></pre>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
